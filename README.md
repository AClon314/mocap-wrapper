# mocap-wrapper
Use with: [mocap_importer](https://github.com/AClon314/mocap_importer_blender)

Platform support:
| ğŸ§Linux | ğŸªŸWindows | ğŸ MacOS | Jupyter Notebook |
| ------ | -------- | ------- | ---------------- |
| ğŸš§      | â“        | â“       | â“                |

A bunch of python scripts that wrap around various mocap libraries to provide a unified interface.  
Only tested on Linux. Not stable yet.

sincerelly thanks to gvhmr/wilor/wilor-mini developers and others that help each otherâ™¥ï¸

## solutions
<details><summary>
stage-by-stage scheme
</summary>

For budget, you can start with a cheap scheme and then upgrade to a more expensive one.

interactable virtual scene & record video & post-calculate â†’ realtime calculate with better GPU â†’ realtime with hardware support

> GPU â†’ iphone â†’ quest3

|                 | minimum                                                 | medium                                                  | higher                                            |
| --------------- | ------------------------------------------------------- | ------------------------------------------------------- | ------------------------------------------------- |
| â™¿ğŸ–¥sit           | ğŸ–±âŒ¨ï¸ğŸ®`user input` as **body** motion/**interact** in game | `GPU`(**face+hand**) +`cam`ğŸ“·                            | UE live link(**face**) +`mini tripod`ğŸ”­            |
| sit cost        | $0 game like VRchat                                     | â‰¥$200 GPU; $0 if use phone as cam                       | $0 live link app; â‰¥$350 iphone; $1 mini tripod    |
| ğŸ§â€â™‚ï¸ğŸ£stand         | `tripod cam`ğŸ”­+ `GPU`(**body+hand**)                     | UE live link(**face**) +Headrig                         | `UE vcam` to **interact**                         |
| stand cost $520 | $20 tripod; â‰¥$200 GPU                                   | $0 live link app; $300 rokoko headrig                   | $0 vcam app                                       |
| ğŸ‘“VR             | `quest3` realtime **hand** mocap & natural **interact** | ~~quest4~~ or `pico 4 pro` or DIY hardware for **face** | `tracker` hardware or `GPU` software for **body** |
| VR cost $850    | $400                                                    | ?                                                       | $450 tracker or â‰¥$200 GPU                         |

</details>

### software:OpenSource
Rank: [bodyğŸ•º](https://paperswithcode.com/task/3d-human-pose-estimation "3Däººä½“å§¿æ€ä¼°è®¡")  [handğŸ‘‹](https://paperswithcode.com/task/3d-hand-pose-estimation "3Dæ‰‹éƒ¨å§¿æ€ä¼°è®¡")  [faceğŸ‘¤](https://paperswithcode.com/task/facial-landmark-detection "é¢éƒ¨ç‰¹å¾ç‚¹æ£€æµ‹") [text to motionæ–‡](https://paperswithcode.com/task/motion-synthesis "è¿åŠ¨åˆæˆ(æ–‡â†’åŠ¨ä½œ)")

| model                                                                                                                                                                                                                       | paper                                                                                                                                    | commit                                                                                                                                                                                                                                                            | issue                                                                                                                                                                                                                                          | comment                                                                      |
| --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------- |
| [âœ…ğŸ•ºGVHMR ![â­](https://img.shields.io/github/stars/zju3dv/GVHMR?style=flat&label=â­)](https://github.com/zju3dv/GVHMR "World-Grounded Human Motion Recovery via Gravity-View Coordinates")                                    | [![citeğŸ™¶](https://api.juleskreuer.eu/citation-badge.php?doi=10.1145/3680528.3687565)](https://doi.org/10.1145/3680528.3687565)           | [![ğŸ•’](https://img.shields.io/github/commit-activity/t/zju3dv/GVHMR/main?label=ğŸ•’) ![LASTğŸ•’](https://img.shields.io/github/last-commit/zju3dv/GVHMR/main?label=ğŸ•’)](https://github.com/zju3dv/GVHMR/commits)                                                          | [![ğŸ¯](https://img.shields.io/github/issues/zju3dv/GVHMR?label=â‰ï¸) ![ğŸ¯close](https://img.shields.io/github/issues-closed/zju3dv/GVHMR?label=â”)](https://github.com/zju3dv/GVHMR/issues)                                                          | 2024, VRAM > 3GB                                                             |
| [ğŸš§ğŸ•ºTRAM ![â­](https://img.shields.io/github/stars/yufu-wang/tram?style=flat&label=â­)](https://github.com/yufu-wang/tram "Global Trajectory and Motion of 3D Humans from in-the-wild Videos")                                 | [![citeğŸ™¶](https://api.juleskreuer.eu/citation-badge.php?doi=10.1007/978-3-031-73247-8_27)](https://doi.org/10.1007/978-3-031-73247-8_27) | [![ğŸ•’](https://img.shields.io/github/commit-activity/t/yufu-wang/tram/main?label=ğŸ•’) ![LASTğŸ•’](https://img.shields.io/github/last-commit/yufu-wang/tram/main?label=ğŸ•’)](https://github.com/yufu-wang/tram/commits)                                                    | [![ğŸ¯](https://img.shields.io/github/issues/yufu-wang/tram?label=â‰ï¸) ![ğŸ¯close](https://img.shields.io/github/issues-closed/yufu-wang/tram?label=â”)](https://github.com/yufu-wang/tram/issues)                                                    | 2024, suit for fast-motion, but VRAM > 6GB                                   |
| [ğŸš§ğŸ•ºCoMotion ![â­](https://img.shields.io/github/stars/apple/ml-comotion?style=flat&label=â­)](https://github.com/apple/ml-comotion "Concurrent Multi-person 3D Motion")                                                       | [![citeğŸ™¶](https://api.juleskreuer.eu/citation-badge.php?doi=10.48550/arXiv.2504.12186)](https://doi.org/10.48550/arXiv.2504.12186)       | [![ğŸ•’](https://img.shields.io/github/commit-activity/t/apple/ml-comotion/main?label=ğŸ•’) ![LASTğŸ•’](https://img.shields.io/github/last-commit/apple/ml-comotion/main?label=ğŸ•’)](https://github.com/apple/ml-comotion/commits)                                           | [![ğŸ¯](https://img.shields.io/github/issues/apple/ml-comotion?label=â‰ï¸) ![ğŸ¯close](https://img.shields.io/github/issues-closed/apple/ml-comotion?label=â”)](https://github.com/apple/ml-comotion/issues)                                           | 2025, belongs to Apple                                                       |
| [ğŸ•’ğŸ•ºSAT-HMR ![â­](https://img.shields.io/github/stars/ChiSu001/SAT-HMR?style=flat&label=â­)](https://github.com/ChiSu001/SAT-HMR "Real-Time Multi-Person 3D Mesh Estimation via Scale-Adaptive Tokens")                        | [![citeğŸ™¶](https://api.juleskreuer.eu/citation-badge.php?doi=10.48550/arXiv.2411.19824)](https://doi.org/10.48550/arXiv.2411.19824)       | [![ğŸ•’](https://img.shields.io/github/commit-activity/t/ChiSu001/SAT-HMR/main?label=ğŸ•’) ![LASTğŸ•’](https://img.shields.io/github/last-commit/ChiSu001/SAT-HMR/main?label=ğŸ•’)](https://github.com/ChiSu001/SAT-HMR/commits)                                              | [![ğŸ¯](https://img.shields.io/github/issues/ChiSu001/SAT-HMR?label=â‰ï¸) ![ğŸ¯close](https://img.shields.io/github/issues-closed/ChiSu001/SAT-HMR?label=â”)](https://github.com/ChiSu001/SAT-HMR/issues)                                              | 2025                                                                         |
| [âœ…ğŸ‘‹WiLoR ![â­](https://img.shields.io/github/stars/rolpotamias/WiLoR?style=flat&label=â­)](https://github.com/rolpotamias/WiLoR "End-to-end 3D hand localization and reconstruction in-the-wild")                             | [![citeğŸ™¶](https://api.juleskreuer.eu/citation-badge.php?doi=10.48550/arXiv.2409.12259)](https://doi.org/10.48550/arXiv.2409.12259)       | [![ğŸ•’](https://img.shields.io/github/commit-activity/t/rolpotamias/WiLoR/main?label=ğŸ•’) ![LASTğŸ•’](https://img.shields.io/github/last-commit/rolpotamias/WiLoR/main?label=ğŸ•’)](https://github.com/rolpotamias/WiLoR/commits)                                           | [![ğŸ¯](https://img.shields.io/github/issues/rolpotamias/WiLoR?label=â‰ï¸) ![ğŸ¯close](https://img.shields.io/github/issues-closed/rolpotamias/WiLoR?label=â”)](https://github.com/rolpotamias/WiLoR/issues)                                           | 2024, use [mini](https://github.com/warmshao/WiLoR-mini), fast, VRAM > 2.5GB |
| [ğŸš§ğŸ‘‹Hamba ![â­](https://img.shields.io/github/stars/humansensinglab/Hamba?style=flat&label=â­)](https://github.com/humansensinglab/Hamba "Single-view 3D Hand Reconstruction withGraph-guided Bi-Scanning Mamba")              | [![citeğŸ™¶](https://api.juleskreuer.eu/citation-badge.php?doi=10.48550/arXiv.2407.09646)](https://doi.org/10.48550/arXiv.2407.09646)       | [![ğŸ•’](https://img.shields.io/github/commit-activity/t/humansensinglab/Hamba/main?label=ğŸ•’) ![LASTğŸ•’](https://img.shields.io/github/last-commit/humansensinglab/Hamba/main?label=ğŸ•’)](https://github.com/humansensinglab/Hamba/commits)                               | [![ğŸ¯](https://img.shields.io/github/issues/humansensinglab/Hamba?label=â‰ï¸) ![ğŸ¯close](https://img.shields.io/github/issues-closed/humansensinglab/Hamba?label=â”)](https://github.com/humansensinglab/Hamba/issues)                               | 2025                                                                         |
| [ğŸ•’ğŸ‘‹OmniHands ![â­](https://img.shields.io/github/stars/LinDixuan/OmniHands?style=flat&label=â­)](https://github.com/LinDixuan/OmniHands)                                                                                      | [![citeğŸ™¶](https://api.juleskreuer.eu/citation-badge.php?doi=10.48550/arXiv.2405.20330)](https://doi.org/10.48550/arXiv.2405.20330)       | [![ğŸ•’](https://img.shields.io/github/commit-activity/t/LinDixuan/OmniHands/main?label=ğŸ•’) ![LASTğŸ•’](https://img.shields.io/github/last-commit/LinDixuan/OmniHands/main?label=ğŸ•’)](https://github.com/LinDixuan/OmniHands/commits)                                     | [![ğŸ¯](https://img.shields.io/github/issues/LinDixuan/OmniHands?label=â‰ï¸) ![ğŸ¯close](https://img.shields.io/github/issues-closed/LinDixuan/OmniHands?label=â”)](https://github.com/LinDixuan/OmniHands/issues)                                     | 2024                                                                         |
| [ğŸ•’ğŸ‘‹HaMeR ![â­](https://img.shields.io/github/stars/geopavlakos/hamer?style=flat&label=â­)](https://github.com/geopavlakos/hamer "Hand Mesh Recovery")                                                                         | [![citeğŸ™¶](https://api.juleskreuer.eu/citation-badge.php?doi=10.48550/arXiv.2312.05251)](https://doi.org/10.48550/arXiv.2312.05251)       | [![ğŸ•’](https://img.shields.io/github/commit-activity/t/geopavlakos/hamer/main?label=ğŸ•’) ![LASTğŸ•’](https://img.shields.io/github/last-commit/geopavlakos/hamer/main?label=ğŸ•’)](https://github.com/geopavlakos/hamer/commits)                                           | [![ğŸ¯](https://img.shields.io/github/issues/geopavlakos/hamer?label=â‰ï¸) ![ğŸ¯close](https://img.shields.io/github/issues-closed/geopavlakos/hamer?label=â”)](https://github.com/geopavlakos/hamer/issues)                                           | 2023                                                                         |
| [ğŸ•’ğŸ‘‹HOISDF ![â­](https://img.shields.io/github/stars/amathislab/hoisdf?style=flat&label=â­)](https://github.com/amathislab/hoisdf "Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields")            | [![citeğŸ™¶](https://api.juleskreuer.eu/citation-badge.php?doi=10.1109/CVPR52733.2024.00989)](https://doi.org/10.1109/CVPR52733.2024.00989) | [![ğŸ•’](https://img.shields.io/github/commit-activity/t/amathislab/hoisdf/main?label=ğŸ•’) ![LASTğŸ•’](https://img.shields.io/github/last-commit/amathislab/hoisdf/main?label=ğŸ•’)](https://github.com/amathislab/hoisdf/commits)                                           | [![ğŸ¯](https://img.shields.io/github/issues/amathislab/hoisdf?label=â‰ï¸) ![ğŸ¯close](https://img.shields.io/github/issues-closed/amathislab/hoisdf?label=â”)](https://github.com/amathislab/hoisdf/issues)                                           | 2024, better on occulusion                                                   |
| [ğŸ•’ğŸ‘¤SPIGA ![â­](https://img.shields.io/github/stars/andresprados/SPIGA?style=flat&label=â­)](https://github.com/andresprados/SPIGA "Shape Preserving Facial Landmarks with Graph Attention Networks")                          | [![citeğŸ™¶](https://api.juleskreuer.eu/citation-badge.php?doi=10.48550/arXiv.2210.07233)](https://doi.org/10.48550/arXiv.2210.07233)       | [![ğŸ•’](https://img.shields.io/github/commit-activity/t/andresprados/SPIGA/main?label=ğŸ•’) ![LASTğŸ•’](https://img.shields.io/github/last-commit/andresprados/SPIGA/main?label=ğŸ•’)](https://github.com/andresprados/SPIGA/commits)                                        | [![ğŸ¯](https://img.shields.io/github/issues/andresprados/SPIGA?label=â‰ï¸) ![ğŸ¯close](https://img.shields.io/github/issues-closed/andresprados/SPIGA?label=â”)](https://github.com/andresprados/SPIGA/issues)                                        | 2022                                                                         |
| [ğŸ•’ğŸ‘¤mediapipe ![â­](https://img.shields.io/github/stars/google-ai-edge/mediapipe?style=flat&label=â­)](https://github.com/google-ai-edge/mediapipe "Cross-platform, customizable ML solutions for live and streaming media. ") | [![citeğŸ™¶](https://api.juleskreuer.eu/citation-badge.php?doi=10.48550/arXiv.2006.10204)](https://doi.org/10.48550/arXiv.2006.10204)       | [![ğŸ•’](https://img.shields.io/github/commit-activity/t/google-ai-edge/mediapipe/master?label=ğŸ•’) ![LASTğŸ•’](https://img.shields.io/github/last-commit/google-ai-edge/mediapipe/master?label=ğŸ•’)](https://github.com/google-ai-edge/mediapipe/commits)                  | [![ğŸ¯](https://img.shields.io/github/issues/google-ai-edge/mediapipe?label=â‰ï¸) ![ğŸ¯close](https://img.shields.io/github/issues-closed/google-ai-edge/mediapipe?label=â”)](https://github.com/google-ai-edge/mediapipe/issues)                      | realtime                                                                     |
| [ğŸ•’æ–‡ğŸµ MotionAnything ![â­](https://img.shields.io/github/stars/steve-zeyu-zhang/MotionAnything?style=flat&label=â­)](https://github.com/steve-zeyu-zhang/MotionAnything)                                                      | [![citeğŸ™¶](https://api.juleskreuer.eu/citation-badge.php?doi=10.48550/arXiv.2503.06955)](https://doi.org/10.48550/arXiv.2503.06955)       | [![ğŸ•’](https://img.shields.io/github/commit-activity/t/steve-zeyu-zhang/MotionAnything/main?label=ğŸ•’) ![LASTğŸ•’](https://img.shields.io/github/last-commit/steve-zeyu-zhang/MotionAnything/main?label=ğŸ•’)](https://github.com/steve-zeyu-zhang/MotionAnything/commits) | [![ğŸ¯](https://img.shields.io/github/issues/steve-zeyu-zhang/MotionAnything?label=â‰ï¸) ![ğŸ¯close](https://img.shields.io/github/issues-closed/steve-zeyu-zhang/MotionAnything?label=â”)](https://github.com/steve-zeyu-zhang/MotionAnything/issues) | 2025, waiting code release                                                   |
| [ğŸ•’æ–‡ momask-codes ![â­](https://img.shields.io/github/stars/EricGuo5513/momask-codes?style=flat&label=â­)](https://github.com/EricGuo5513/momask-codes)                                                                       | [![citeğŸ™¶](https://api.juleskreuer.eu/citation-badge.php?doi=10.48550/arXiv.2312.00063)](https://doi.org/10.48550/arXiv.2312.00063)       | [![ğŸ•’](https://img.shields.io/github/commit-activity/t/EricGuo5513/momask-codes/main?label=ğŸ•’) ![LASTğŸ•’](https://img.shields.io/github/last-commit/EricGuo5513/momask-codes/main?label=ğŸ•’)](https://github.com/EricGuo5513/momask-codes/commits)                      | [![ğŸ¯](https://img.shields.io/github/issues/EricGuo5513/momask-codes?label=â‰ï¸) ![ğŸ¯close](https://img.shields.io/github/issues-closed/EricGuo5513/momask-codes?label=â”)](https://github.com/EricGuo5513/momask-codes/issues)                      | 2024                                                                         |
- hand: no constant tracking for video(just no yolo, ready for photo but not video)

### software:non-OpenSource
- [ğŸ•ºğŸ‘‹ğŸ‘¤-æ–‡ğŸµ Genmo ï¼ˆNvidia Labï¼‰](https://research.nvidia.com/labs/dair/genmo/)
- [ğŸ•ºğŸ‘‹ğŸ‘¤Look Ma, no markers: holistic performance capture without the hassle](https://www.youtube.com/watch?v=4RkLDW3GmdY)
- [ğŸ‘¤D-ViT](https://arxiv.org/abs/2411.07167v1 "Cascaded Dual Vision Transformer for Accurate Facial Landmark Detection")

### hardware:RealTime
|           | Solution                                                                                                                                                                           | Comment       |
| --------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------- |
| ğŸ‘¤face     | [ğŸiFacialMocap](https://www.ifacialmocap.com/) (iPhone X + PC(win/Mac)) <br> [ğŸ¤–Meowface](https://suvidriel.itch.io/meowface) (free, Android, can work with iFacialMocap PC client) | Shape key     |
|           | [ğŸ+ğŸ’»Unreal Live Link](https://dev.epicgames.com/documentation/en-us/unreal-engine/live-link-in-unreal-engine)                                                                      | Bone          |
| hand/body | VR headset or VR trackers                                                                                                                                                          | ~~Off topic~~ |
- *ğŸ`iphoneâ‰¥X(12/13 best)`for **better face mocap result** on UE live link, though you can use androidğŸ¤– to do live link.*

## install å®‰è£…
```sh
# sudo -i; bash <(curl -sSL https://gitee.com/SuperManito/LinuxMirrors/raw/main/ChangeMirrors.sh) # ä¸€é”®è®¾ç½®linuxé•œåƒ(å¯é€‰)
curl https://raw.githubusercontent.com/AClon314/mocap-wrapper/refs/heads/master/src/mocap_wrapper/script/mamba.py | python
mocap --install -b gvhmr,wilor
```

This will create new conda env `nogil` for mocap-wrapper, and will install mocap/wilor in `mocap` env. Manually like this:
```sh
mamba env create -n nogil python-freethreading
mamba activate nogil
pip install git+https://github.com/AClon314/mocap-wrapper.git

# If you want to debug run/gvhmr.py
mamba activate mocap
./gvhmr.py
```

## usage ç”¨æ³•
See `mocap -h` for more options.
```sh
mocap -i input.mp4
mocap -i input.mp4 -b gvhmr,wilor -o outdir
```

## dev å¼€å‘
You have to read these if you want to modify code.

```sh
LOG=debug mocap -I
```

### [docker](docker/Dockerfile)
```sh
# docker build -t mocap -f docker/Dockerfile .
podman build -t mocap -f docker/Dockerfile . --security-opt label=disable
# github action local
act -j test -v --action-offline-mode --bind --reuse --env LOG=D # --rm=false
```

### requirements
Will do the following steps for each requirements.txt in [`src/mocap_wrapper/requirements`](src/mocap_wrapper/requirements/gvhmr.txt):

1. use `mamba`/`conda` to install, so the commented lines are not installed.
2. use `pip` to install the **rest** packages that startwith `# `. So if you want to do comments, you can use `## `or`#...`without space char.

### .npz struct
key: `Armature mapping from`;`Algorithm run`;`who`;`begin`;`prop[0]`;`prop[1]`...

example: 
- smplx;gvhmr;person0;0;body_pose = array([...], dtype=...)
- smplx;wilor;person1;9;hand_pose = ...
- smplx;wilor;person1;1;bbox = ...

ps: the blender addon use *Armature mapping **to***

#### prop[0]
- pose: thetas, Î¸
- betas: shape, Î²
- expression: psi, Ïˆ
- trans(lation) å¹³ç§»
- global_orient: rotate æ—‹è½¬
- bbox: yolo_bbox_xyXY

## Licenses åè®®
By using this repository, you must also comply with the terms of these external licenses:

| Repo          | License                                                                                                                                                                                                                                              |
| ------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| GVHMR         | [Copyright 2022-2023 3D Vision Group at the State Key Lab of CAD&CG, Zhejiang University. All Rights Reserved. ![CC BY-NC-SA](https://licensebuttons.net/l/by-nc-sa/3.0/88x31.png)](https://github.com/zju3dv/GVHMR/blob/main/LICENSE "CC BY-NC-SA") |
| WiLoR         | [![CC BY-NC-ND 4.0](https://licensebuttons.net/l/by-nc-nd/3.0/88x31.png)](https://github.com/rolpotamias/WiLoR/blob/main/license.txt "CC BY-NC-ND 4.0")                                                                                              |
| mocap-wrapper | [AGPL v3](./LICENSE)                                                                                                                                                                                                                                 |